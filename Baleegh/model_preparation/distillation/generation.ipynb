{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Configure any model from HF HUB\n",
    "# assert input(\"YOU WILL REMOVE THE HUB MODEL FOR THIS, TYPE 'OK' TO PROCEED: \").upper() == 'OK'\n",
    "# model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "# model_name = \"facebook/m2m100_1.2B\"\n",
    "# #model_name= \"Helsinki-NLP/opus-mt-en-ar\"\n",
    "# model_name= \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# generation_config = GenerationConfig(\n",
    "#     temperature=0.5,\n",
    "#     do_sample=True,\n",
    "#     max_length=256,\n",
    "#     forced_bos_token_id = 256011, # Arabic\n",
    "\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     bos_token_id= 256011,\n",
    "#     decoder_start_token_id= 2,\n",
    "#     eos_token_id= tokenizer.eos_token_id,\n",
    "    \n",
    "# #     num_beams = 4,\n",
    "# #     early_stopping=True,\n",
    "# #     top_k=50,\n",
    "    \n",
    "# #     renormalize_logits=True,\n",
    "    \n",
    "# #     # Testing Config\n",
    "# #       repetition_penalty=0.5,\n",
    "# #     num_return_sequences=4, # Number of sentences to generate\n",
    "# #     return_dict_in_generate=True, # Returns the complete generation data from within the model.\n",
    "# #     output_scores=True, # Score of each token.\n",
    "# )\n",
    "\n",
    "# tokenizer.src_lang=\"eng_Latn\"\n",
    "# tokenizer.tgt_lang=\"arb_Arab\"\n",
    "\n",
    "# model.push_to_hub(\"Abdulmohsena/Faseeh_LoRA\")\n",
    "# tokenizer.push_to_hub(\"Abdulmohsena/Faseeh_LoRA\")\n",
    "# generation_config.push_to_hub(\"Abdulmohsena/Faseeh_LoRA\")\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device),\n",
    "        temperature=0.7,\n",
    "        top_p=0.85, # Gets only the top 85% probability tokens (Creative but deterministic)\n",
    "        length_penalty = 0, # Favors shorter translations\n",
    "        num_beams = 4,\n",
    "        num_return_sequences=4,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
